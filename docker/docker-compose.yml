version: '3.4'
services:
  base_image:
    build:
      context: .
      dockerfile: docker/base_image/Dockerfile
    image: datapipeline_base_image
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
  namenode:
    image: uhopper/hadoop-namenode
    hostname: namenode
    container_name: namenode
    domainname: hadoop
    networks:
      - hadoop
    volumes:
      - namenode:/hadoop/dfs/name
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_replication=1
      - CLUSTER_NAME=data_pipeline_cluster
    ports:
      - "50070:50070"
    depends_on:
      - datanode1
  datanode1:
    image: uhopper/hadoop-datanode
    hostname: datanode1
    container_name: datanode1
    domainname: hadoop
    networks:
      - hadoop
    volumes:
      - datanode:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_replication=1
      - CLUSTER_NAME=data_pipeline_cluster
    ports:
      - "50075:50075"
  gateway:
    build:
      context: .
      dockerfile: docker/gateway/Dockerfile
    hostname: gateway
    container_name: gateway
    domainname: hadoop
    networks:
      - hadoop
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - broker
      - datanode1
      - namenode
  broker:
    image: wurstmeister/kafka
    hostname: broker
    container_name: broker
    domainname: hadoop
    networks:
      - hadoop
    depends_on:
      - zookeeper
    environment:
      - KAFKA_CREATE_TOPICS=twitter:1:1,channel:1:1
      - KAFKA_ADVERTISED_HOST_NAME=broker
      - KAFKA_ADVERTISED_PORT=9092
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ZOOKEEPER_TIMEOUT_MS=30000
  zookeeper:
    image: zookeeper
    hostname: zookeeper
    container_name: zookeeper
    domainname: hadoop
    networks:
      - hadoop
    restart: always
  flume:
    build:
      context: .
      dockerfile: docker/flume/Dockerfile
    hostname: flume
    container_name: flume
    domainname: hadoop
    networks:
      - hadoop
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - broker
      - datanode1
      - namenode
  nodemanager:
    image: uhopper/hadoop-nodemanager
    hostname: nodemanager1
    container_name: nodemanager1
    domainname: hadoop
    networks:
      - hadoop
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - YARN_CONF_yarn_log___aggregation___enable=true
      - YARN_CONF_yarn_nodemanager_remote___app___log___dir=/app-logs
    depends_on:
      - namenode
  resourcemanager:
    image: uhopper/hadoop-resourcemanager
    hostname: resourcemanager
    container_name: resourcemanager
    domainname: hadoop
    networks:
      - hadoop
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_log___aggregation___enable=true
    depends_on:
      - namenode
  prometheus:
    build:
      context: .
      dockerfile: docker/prometheus/Dockerfile
    hostname: prometheus
    container_name: prometheus
    domainname: hadoop
    networks:
      - hadoop
    depends_on:
      - broker
      - zookeeper
      - flume
      - gateway
      - datanode1
      - namenode
      - kafka_exporter
    ports:
      - "9090:9090"
  kafka_exporter:
    image: danielqsj/kafka-exporter
    command: --kafka.server=broker:9092 --web.listen-address=0.0.0.0:1337
    hostname: kafka_exporter
    container_name: kafka_exporter
    domainname: hadoop
    networks:
      - hadoop
    depends_on:
      - broker
      - zookeeper
    ports:
      - "1337:1337"
    restart: on-failure:30
networks:
  hadoop:
volumes:
  namenode:
  datanode:
