version: '3.4'
services:
  base_image:
    build:
      context: .
      dockerfile: docker/base_image/Dockerfile
    image: v3nd3774/data_pipeline:base_image
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
  namenode:
    image: uhopper/hadoop-namenode
    hostname: namenode
    container_name: namenode
    domainname: hadoop
    networks:
      - hadoop
    volumes:
      - namenode:/hadoop/dfs/name
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_replication=1
      - CLUSTER_NAME=data_pipeline_cluster
    ports:
      - "50070:50070"
    depends_on:
      - datanode1
  datanode1:
    image: uhopper/hadoop-datanode
    hostname: datanode1
    container_name: datanode1
    domainname: hadoop
    networks:
      - hadoop
    volumes:
      - datanode:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_replication=1
      - CLUSTER_NAME=data_pipeline_cluster
    ports:
      - "50075:50075"
  gateway:
    build:
      context: .
      dockerfile: docker/gateway/Dockerfile
    image: v3nd3774/data_pipeline:gateway
    hostname: gateway
    container_name: gateway
    domainname: hadoop
    networks:
      - hadoop
      - nonhadoop
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - broker
      - datanode1
      - namenode
    restart: always
  broker:
    image: wurstmeister/kafka
    hostname: broker
    container_name: broker
    domainname: hadoop
    networks:
      - nonhadoop
    depends_on:
      - zookeeper
    environment:
      - KAFKA_CREATE_TOPICS=twitter:1:1,channel:1:1
      - KAFKA_ADVERTISED_HOST_NAME=broker
      - KAFKA_ADVERTISED_PORT=9092
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ZOOKEEPER_TIMEOUT_MS=30000
  zookeeper:
    image: zookeeper
    hostname: zookeeper
    container_name: zookeeper
    domainname: hadoop
    networks:
      - nonhadoop
    restart: always
  flume:
    build:
      context: .
      dockerfile: docker/flume/Dockerfile
    image: v3nd3774/data_pipeline:flume
    hostname: flume
    container_name: flume
    domainname: hadoop
    networks:
      - hadoop
      - nonhadoop
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - broker
      - datanode1
      - namenode
  prometheus:
    build:
      context: .
      dockerfile: docker/prometheus/Dockerfile
    image: v3nd3774/data_pipeline:prometheus
    hostname: prometheus
    container_name: prometheus
    domainname: hadoop
    networks:
      - nonhadoop
    depends_on:
      - broker
      - zookeeper
      - flume
      - gateway
      - datanode1
      - namenode
      - kafka_exporter
    ports:
      - "9090:9090"
  kafka_exporter:
    image: danielqsj/kafka-exporter
    command: --kafka.server=broker:9092 --web.listen-address=0.0.0.0:1337
    hostname: kafka_exporter
    container_name: kafka_exporter
    domainname: hadoop
    networks:
      - nonhadoop
    depends_on:
      - broker
      - zookeeper
    ports:
      - "1337:1337"
    restart: always
  hdfs_directory_exporter:
    build:
      context: .
      dockerfile: docker/hdfs_directory_exporter/Dockerfile
    image: v3nd3774/data_pipeline:hdfs_directory_exporter
    hostname: hdfs_directory_exporter
    container_name: hdfs_directory_exporter
    domainname: hadoop
    networks:
      - hadoop
      - nonhadoop
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - datanode1
      - namenode
    ports:
      - "42069:42069"
    restart: always
  grafana:
    build:
      context: .
      dockerfile: docker/grafana/Dockerfile
    image: v3nd3774/data_pipeline:grafana
    hostname: grafana
    container_name: grafana
    domainname: hadoop
    networks:
      - nonhadoop
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=michael
      - GF_SECURITY_ADMIN_PASSWORD=myers
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - kafka_exporter
      - prometheus
networks:
  hadoop:
  nonhadoop:
volumes:
  namenode:
  datanode:
